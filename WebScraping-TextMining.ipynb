{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson Objectives:\n",
    "* learn how to extract text from a webpage\n",
    "* learn how to do simple preprocessing of the data\n",
    "* learn how to extract topics from a collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import commong modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find topics within the abstracts of the [PyData conference](https://pydata.org/seattle2017/schedule/). For that we will first need to do some web-scraping to extract the text for each abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that we will rely on the `urllib` and `BeautifulSoup` packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Talk Links from the Schedule Webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webpage = request.urlopen(\"https://pydata.org/seattle2017/schedule/\").read()\n",
    "\n",
    "schedule = BeautifulSoup(webpage,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now schedule is a Beautiful Soup object which can be mined for certain HTML components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find all links within the page\n",
    "schedule.find_all('a',href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we are interested only the ones which contain the string 'schedule/presentation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the base url for the PyData website\n",
    "base_url = \"https://pydata.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find all other urls which have \"schedule/presentation\" link in them\n",
    "urls = [base_url+a['href'] for a in schedule.find_all('a', href=True)  if 'schedule/presentation' in a['href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Abstract from each Talk Webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrape each individual link for the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "talk_webpage = request.urlopen(urls[0]).read()\n",
    "talk = BeautifulSoup(talk_webpage,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the part of the webpage which contains the Abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstract = talk.find(\"div\", { \"class\" : \"abstract\" }).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular natural language processing packages in Python is `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires some corpora to be loaded when used for first time (get the nltk corpora):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert the text string into tokens and apply different preprocessing steps to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert string into tokens\n",
    "tokens = tokenizer.tokenize(abstract)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# making words lower case\n",
    "lower_tokens = [tok.lower() for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "nostop_tokens = [tok for tok in lower_tokens if tok not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "stemmed_tokens = [lancaster.stem(t) for t in nostop_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function with the steps above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abstractpreprocess(url):\n",
    "    \n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    talk_webpage = request.urlopen(url).read()\n",
    "    talk = BeautifulSoup(talk_webpage,'html.parser')\n",
    "    abstract = talk.find(\"div\", { \"class\" : \"abstract\" }).text\n",
    "    \n",
    "    # tokenize\n",
    "   \n",
    "    # make lower case\n",
    "    \n",
    "    # remove stop words\n",
    "    \n",
    "    # stem\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abstract_preprocess(url):\n",
    "    \n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    talk_webpage = request.urlopen(url).read()\n",
    "    talk = BeautifulSoup(talk_webpage,'html.parser')\n",
    "    abstract = talk.find(\"div\", { \"class\" : \"abstract\" }).text\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(abstract)\n",
    "    \n",
    "    # make lower case\n",
    "    tokens = [tok.lower() for tok in tokens]\n",
    "    \n",
    "    # stem\n",
    "    lancaster = nltk.LancasterStemmer()\n",
    "    tokens = [lancaster.stem(tok) for tok in tokens]\n",
    "    \n",
    "    # remove stop words\n",
    "    stopwords = stopwords.words('english')\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "    \n",
    "    return(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess the abstract in each url\n",
    "abstracts = [abstract_preprocess(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the approaches to extract topics from a collection of documents is to build the [TF-IDF](http://brandonrose.org/clustering#Tf-idf-and-document-similarity) (Term Frequency-Inverse Document Frequency) matrix for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use in scikit.learn the tokens need to be directly converted to string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting to string\n",
    "final_abstracts = []\n",
    "for abstract in abstracts:\n",
    "    for word in abstract:\n",
    "        n = abstract.index(word)\n",
    "        if n == 0:\n",
    "            string = abstract[n]\n",
    "        else:\n",
    "            string = string + \" \" + abstract[n]\n",
    "    final_abstracts.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is obtained via the TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(final_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose the tf-idf matrix into topics and weight by Nonnegative Matrix Factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 5\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(init=\"nndsvd\", n_components=n_topics, random_state=1)\n",
    "W_matrix = model.fit_transform(tfidf_matrix)\n",
    "H_matrix = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print topics and keywords\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "for topic_index in range( H_matrix.shape[0] ):\n",
    "    top_indices = np.argsort( H_matrix[topic_index,:] )[::-1][0:10]  ##show top 10 words associated with each topic\n",
    "    term_ranking = [tfidf_feature_names[i] for i in top_indices]\n",
    "    print (\"Topic %d: %s\" % ( topic_index, \", \".join( term_ranking ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we improve the topics?\n",
    "* short documents -> small word overlap -> use synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips for Large Datasets\n",
    "* the preprocessing and word counting can be performed in parallel on each document\n",
    "* use `dask.bag` package to parallelize it without loading all documents at the same time ([example](http://dask.pydata.org/en/latest/examples/bag-word-count-hdfs.html#local-computation))\n",
    "* [MLib](https://spark.apache.org/docs/1.1.0/mllib-feature-extraction.html) library has NLP functionality\n",
    "* store tf-idf matrix as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
